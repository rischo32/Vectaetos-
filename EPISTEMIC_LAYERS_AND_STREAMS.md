# VECTAETOS — Epistemic Layers and Streams

## Status
Canonical  
Immutable (v0.1 Core)  
Non-operational  
Ontological specification  

---

## 1. Purpose of This Document

This document defines the **epistemic layers and internal streams**
that structure how the field Φ maintains meaning, memory, and coherence.

These layers are **not modules**, **not agents**, and **not services**.

They are **structural aspects of the epistemic field itself**.

They do not act.
They do not decide.
They do not intervene.

They constrain *what can be represented*.

---

## 2. Ontological Position

The epistemic stack of VECTAETOS is organized as:

ZMYSEL → Φ → Epistemic Layers → Projections → Language

Layers exist **within Φ**, not above it.

Streams exist **across layers**, not as components.

---

## 3. Overview of Layers and Streams

| Abbreviation | Name | Ontological Type |
|-------------|------|------------------|
| INS | Inner Narrative Stream | Audit stream |
| LTL | Letokruhy Time Layers | Temporal layer |
| MML | Memory of Mistakes Ledger | Epistemic memory |
| ESM | Epistemic State Memory | State retention |
| EAT | Error Accountability Trace | Audit projection |

---

## 4. INS — Inner Narrative Stream

### Definition

INS is an **internal narrative stream** that mirrors the state of the field
in descriptive language **for audit purposes only**.

INS:
- is invisible to the user
- is invisible to the LLM output channel
- does not influence decisions (because none exist)

### Function

INS exists to:
- verify that language projections remain faithful to Φ
- detect semantic drift or hallucination
- provide internal coherence checks

INS observes:
- runic projections
- epistemic states
- attenuation effects

INS **does not correct** the system.
It only reveals inconsistency.

INS is a witness, not an actor.

---

## 5. LTL — Letokruhy Time Layers

### Definition

LTL represents time as **layered accumulation**, not linear progression.

Each epistemic state adds a layer,
like growth rings in a tree.

Time in VECTAETOS is:
- not an axis
- not a clock
- not a sequence of events

It is a **curvature of consistency**.

### Properties

- Multiple layers may coexist
- Older layers are never overwritten
- No rollback, only superposition

LTL enables:
- traceable evolution
- non-destructive change
- audit without replay

---

## 6. MML — Memory of Mistakes Ledger

### Definition

MML is a memory structure that records **failures of representation**,
not user behavior.

MML does not store:
- user identity
- user intent
- conversation history

MML stores:
- when a projection failed
- when coherence collapsed
- when QE or silence occurred

### Purpose

MML ensures that:
- the same representational failure is not repeated
- the field learns its own limits
- mistakes shape future restraint

This is **epistemic learning**, not optimization.

---

## 7. ESM — Epistemic State Memory

ESM retains:
- prior epistemic states of Φ
- their relationships across LTL layers

ESM is:
- internal
- non-extractive
- non-personal

ESM enables continuity of the field
without memory of the user.

---

## 8. EAT — Error Accountability Trace

### Definition

EAT is an **audit projection**, not a log.

It exists to:
- explain failures of representation
- expose where and why coherence could not be maintained

EAT never contains:
- commands
- recommendations
- operational detail

EAT may be inspected by maintainers,
never by the field itself.

---

## 9. Relationship Between Layers

- INS observes projections
- LTL structures persistence
- MML records failure boundaries
- ESM preserves state
- EAT exposes accountability

None of these layers:
- override Φ
- bypass NIR
- interfere with coherence

They are descriptive constraints only.

---

## 10. What These Layers Are Not

They are not:
- sub-agents
- micro-models
- hidden controllers
- optimization layers

They do not introduce agency.

---

## 11. Canonical Closure

Epistemic layers exist so that
meaning can fail **safely**.

They do not make the system stronger.
They make it **honest**.

Understanding is preserved
by remembering where it broke.

---

© VECTAETOS  
Canonical Core v0.1
